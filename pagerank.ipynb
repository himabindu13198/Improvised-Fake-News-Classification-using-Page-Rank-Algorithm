{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()", "outputs": [], "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "from pyspark.sql.types import StringType, ArrayType, LongType\nfrom pyspark.sql.functions import col,udf,lower\nimport re\nfrom pyspark.sql.functions import explode", "outputs": [], "metadata": {}}, {"source": "#START start from here", "cell_type": "markdown", "metadata": {}}, {"execution_count": 103, "cell_type": "code", "source": "df2 = spark.read.csv(\"gs://cluster-new/tmp_output/result.csv\",header=True,mode=\"DROPMALFORMED\")", "outputs": [], "metadata": {}}, {"execution_count": 105, "cell_type": "code", "source": "df2.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+\n|                From|                  To|\n+--------------------+--------------------+\n|https://www.nytim...|https://www.reute...|\n|https://www.nytim...|https://www.reute...|\n|https://www.nytim...|https://www.nytim...|\n|https://www.nytim...|https://www.nytim...|\n|https://www.nytim...|https://www.nytim...|\n|https://www.nytim...|https://www.nytim...|\n|https://www.nytim...|https://www.nytim...|\n|https://www.nytim...|https://www.nytim...|\n|https://www.activ...|http://www.cnn.co...|\n|https://www.activ...|https://www.reute...|\n|https://www.activ...|https://www.reute...|\n|https://www.activ...|http://www.cnn.co...|\n|https://www.activ...|http://www.cnn.co...|\n|https://www.activ...|https://www.nytim...|\n|https://www.activ...|https://www.reute...|\n|https://www.nytim...|http://beforeitsn...|\n|https://www.nytim...|http://beforeitsn...|\n|https://www.nytim...|http://www.bbc.co...|\n|http://dailybuzzl...|http://beforeitsn...|\n|http://dailybuzzl...|http://beforeitsn...|\n+--------------------+--------------------+\nonly showing top 20 rows\n\n"}], "metadata": {}}, {"execution_count": 106, "cell_type": "code", "source": "df = df2\n# df.select('From').collect()", "outputs": [], "metadata": {}}, {"execution_count": 107, "cell_type": "code", "source": "df_rdd = df.rdd.map(tuple)\n# df_rdd.collect()", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "df_rdd.count()", "outputs": [], "metadata": {}}, {"execution_count": 108, "cell_type": "code", "source": "df_rdd = df_rdd.groupByKey()", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "df_rdd.count()", "outputs": [], "metadata": {}}, {"execution_count": 109, "cell_type": "code", "source": "# df_rdd.collect()", "outputs": [], "metadata": {}}, {"execution_count": 110, "cell_type": "code", "source": "#this iterable dataframe might have 0 length list\ndf_rdd_filtered = df_rdd.filter(lambda x: len(x[1]) != 0)\n\n# temp_df = sqlContext.createDataFrame(df_rdd.collect(), ['title', 'neighbors'])\n# temp_df = temp_df.filter(len('neighbors') != 0)\n# temp_df.show()", "outputs": [], "metadata": {}}, {"execution_count": 111, "cell_type": "code", "source": "df_rdd_filtered.count()", "outputs": [{"execution_count": 111, "output_type": "execute_result", "data": {"text/plain": "528"}, "metadata": {}}], "metadata": {}}, {"execution_count": 112, "cell_type": "code", "source": "#trying to get lenght of RHS column list\nrdd_length = df_rdd.map(lambda x: len(x[1]))\n# rdd_length.collect()", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# temp_df.count()", "outputs": [], "metadata": {}}, {"execution_count": 113, "cell_type": "code", "source": "#test\n# def test(df_rdd):\n#     test_df = sqlContext.createDataFrame(df_rdd.collect(), ['title', 'rank'])\n#     test_df.orderBy(\"rank\").show()\n\n# #passed\n# test(df_rdd_filtered)", "outputs": [], "metadata": {}}, {"execution_count": 114, "cell_type": "code", "source": "#initalize rank\n\nranks = df_rdd.map(lambda neighbors: (neighbors[0], 1.0))  \n# ranks.collect()", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# ranks.count()", "outputs": [], "metadata": {}}, {"execution_count": 115, "cell_type": "code", "source": "def calc_contrib(links, rank):\n    num = len(links)\n    for link in links:\n        if num == 0:\n            yield (link,0)\n        else:\n            yield (link, rank / num)", "outputs": [], "metadata": {}}, {"execution_count": 123, "cell_type": "code", "source": "from operator import add\nfor i in range(1):\n        # Calculates contributions\n        contribs = df_rdd.join(ranks).flatMap(lambda (link, (links, rank)): calc_contrib(links, rank))\n            # \u201cx\u201d means (key, (value list, 1.0)), therefore x[1][0] is the \u201cvalue list\u201d and x[1][1] is \u201c1.0\u201d. \n            #This function computes how much pagerank score a node receives from its neighbor.\n             #reduceByKey(add) sums up the values for one key\n        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)  ", "outputs": [], "metadata": {}}, {"execution_count": 124, "cell_type": "code", "source": "# contribs.collect()  #contributions\ncontribs = df_rdd.join(ranks).flatMap(lambda x: calc_contrib(x[1][0], x[1][1]))\n# test(contribs)", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# contribs.count()", "outputs": [], "metadata": {}}, {"execution_count": 125, "cell_type": "code", "source": "#converting to df\nl = ranks.collect()\ndf1 = sqlContext.createDataFrame(l, ['title', 'rank'])\ndf1.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+-------------------+\n|               title|               rank|\n+--------------------+-------------------+\n|http://www.bbc.co...|0.18207756003517162|\n|https://www.reute...|0.21144579119615872|\n|http://www.disclo...| 0.2111714958572234|\n|http://www.cnn.co...| 0.1886914903975838|\n|https://www.reute...|0.19019839904056873|\n|http://beforeitsn...|0.21144578874816614|\n|https://www.nytim...|0.23015827010706763|\n|http://money.cnn....|0.18626627179734084|\n|http://www.cnn.co...|0.17656535565436474|\n|http://www.cnn.co...|0.22936666632909403|\n|https://www.nytim...|0.17991601737281537|\n|http://beforeitsn...| 0.2098319612890629|\n|http://abcnews.go...|0.18626627179734084|\n|https://www.reute...|0.17983093184786958|\n|http://inhealth.c...|0.18207756003517162|\n|https://www.nytim...|0.19874064773778782|\n|https://www.reute...|0.18072289682207565|\n|http://www.bbc.co...|0.18222145807184875|\n|https://www.reute...|0.17983092932280006|\n|http://www.bbc.co...|0.17991598064453146|\n+--------------------+-------------------+\nonly showing top 20 rows\n\n"}], "metadata": {}}, {"execution_count": 128, "cell_type": "code", "source": "# save the entire csv file\ndf1.repartition(1).write.csv(\"gs://new_cluster/pagerank\")", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pyspark", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.14", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}